{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm, metrics, tree, decomposition, svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edited_pipeline as ep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(100, 2))\n",
    "msk = np.random.rand(len(df)) < 0.8\n",
    "train = df[msk]\n",
    "test = df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_sc(y_test_sorted,y_pred_probs_sorted):\n",
    "    return roc_auc_score(y_test_sorted,y_pred_probs_sorted)\n",
    "\n",
    "def joint_sort_descending(l1, l2):\n",
    "    # l1 and l2 have to be numpy arrays\n",
    "\n",
    "    '''\n",
    "    Order the specified arrays into descending (see which ones got the highest score)\n",
    "\n",
    "    Intputs: \n",
    "        l1: first array\n",
    "        l2: second array\n",
    "    Returns: \n",
    "        The sorted data\n",
    "    '''\n",
    "    idx = np.argsort(l1)[::-1]\n",
    "    return l1[idx], l2[idx]\n",
    "\n",
    "def generate_binary_at_k(y_scores, k):\n",
    "    '''\n",
    "    Makes the cutoff for keeping only the observations that are within the top k%\n",
    "    Input:\n",
    "        y_scores: Predicted label\n",
    "        k: threshold for keeping the points\n",
    "    '''\n",
    "    cutoff_index = int(len(y_scores) * (k / 100.0))\n",
    "    test_predictions_binary = [1 if x < cutoff_index else 0 for x in range(len(y_scores))]\n",
    "    return test_predictions_binary\n",
    "\n",
    "def precision_at_k(y_true, y_scores, k):\n",
    "    y_scores, y_true = joint_sort_descending(np.array(y_scores), np.array(y_true))\n",
    "    preds_at_k = generate_binary_at_k(y_scores, k)\n",
    "    #precision, _, _, _ = metrics.precision_recall_fscore_support(y_true, preds_at_k)\n",
    "    #precision = precision[1]  # only interested in precision for label 1\n",
    "    precision = precision_score(y_true, preds_at_k)\n",
    "    return precision\n",
    "\n",
    "def recall_at_k(y_true, y_scores, k):\n",
    "    #y_scores_sorted, y_true_sorted = zip(*sorted(zip(y_scores, y_true), reverse=True))\n",
    "    y_scores_sorted, y_true_sorted = joint_sort_descending(np.array(y_scores), np.array(y_true))\n",
    "    preds_at_k = generate_binary_at_k(y_scores_sorted, k)\n",
    "    #precision, _, _, _ = metrics.precision_recall_fscore_support(y_true, preds_at_k)\n",
    "    #precision = precision[1]  # only interested in precision for label 1\n",
    "    recall = recall_score(y_true_sorted, preds_at_k)\n",
    "    return recall\n",
    "\n",
    "def f1_at_k(y_true, y_scores, k):\n",
    "    '''\n",
    "    Calculates the F1 score of a determined k% testing sample\n",
    "\n",
    "    Inputs:\n",
    "        y_true: label from test data\n",
    "        y_scores: predicted label for test data\n",
    "        k: k%, the % value we want to predict\n",
    "\n",
    "    Returns: \n",
    "        F1 score  \n",
    "    '''\n",
    "    y_scores_sorted, y_true_sorted = joint_sort_descending(np.array(y_scores), np.array(y_true))\n",
    "    preds_at_k = generate_binary_at_k(y_scores_sorted, k)\n",
    "    recall = recall_score(y_true_sorted, preds_at_k)\n",
    "    precision = precision_score(y_true_sorted, preds_at_k)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return F1\n",
    "\n",
    "def accurate(y_test_sorted_,y_pred_sorted):\n",
    "    '''\n",
    "    Calculates the accuracy score of a determined k% testing sample\n",
    "\n",
    "    Inputs:\n",
    "        y_test_sorted: label from test data\n",
    "        y_pred_sorted: predicted label for test data\n",
    "\n",
    "    Returns: \n",
    "        accuracy score  \n",
    "    '''\n",
    "\n",
    "    accure = metrics.accuracy_score(y_test_sorted_,y_pred_sorted)\n",
    "    return accure\n",
    "\n",
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "\n",
    "    '''\n",
    "    Plots the precision-recall sample for the specified model\n",
    "\n",
    "    Inputs:\n",
    "        y_test_sorted_: label from test data\n",
    "        y_pred_sorted: predicted label for test data\n",
    "\n",
    "    Returns: \n",
    "        Image with the precision recall curve\n",
    "    '''\n",
    "\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    \n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax2.set_xlim([0,1])\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    #plt.savefig(name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_roc(name, probs, true, output_type):\n",
    "    '''\n",
    "    Plots the AUC for the specified model\n",
    "\n",
    "    Input:\n",
    "        name(string): name we want to assign to the model\n",
    "        probs(array): predicted label for test data\n",
    "        true(array): label for true data\n",
    "        output_types()\n",
    "    \n",
    "    '''\n",
    "    fpr, tpr, thresholds = roc_curve(true, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    pl.clf()\n",
    "    pl.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    pl.plot([0, 1], [0, 1], 'k--')\n",
    "    pl.xlim([0.0, 1.05])\n",
    "    pl.ylim([0.0, 1.05])\n",
    "    pl.xlabel('False Positive Rate')\n",
    "    pl.ylabel('True Positive Rate')\n",
    "    pl.title(name)\n",
    "    pl.legend(loc=\"lower right\")\n",
    "    if (output_type == 'save'):\n",
    "        plt.savefig(name)\n",
    "    elif (output_type == 'show'):\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(metric_lst, k_lst,y_test_sorted,y_pred_probs_sorted):\n",
    "    '''\n",
    "    Creates a list of the evaluation metrics especified. \n",
    "\n",
    "    Inputs: \n",
    "        metric_lst (list)\n",
    "        precision_lst(list)\n",
    "        y_test_sorted(array)\n",
    "        y_pred_probs(array)\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    final_lst = []\n",
    "    if roc_auc_score in metric_lst:\n",
    "        metric_lst.remove(roc_auc_score)\n",
    "        final_lst.append(roc_auc_score(y_test_sorted,y_pred_probs_sorted))\n",
    "        \n",
    "    for metr in metric_lst:\n",
    "        for prec in k_lst:\n",
    "            final_lst.append(metr(y_test_sorted,y_pred_probs_sorted,prec))\n",
    "\n",
    "    return final_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_lst =[roc_auc_sc,precision_at_k,recall_at_k,f1_at_k]\n",
    "k_lst = [1.0,2.0,5.0,10.0,20.0,30.0,40.0,50.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
